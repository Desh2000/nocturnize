{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ViYFyDLPwZL"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# === Part 1: The Environment (EVChargingEnv)\n",
        "# ==============================================================================\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "# Based on the project report \n",
        "EV_BATTERY_CAPACITY_KWH = 40.0  # kWh, based on a common EV model like the Nissan Leaf \n",
        "CHARGER_POWER_KW = 7.4          # kW \n",
        "# Price Schedule based on CEB Time-of-Day tariffs \n",
        "PEAK_PRICE_PER_KWH = 0.30\n",
        "OFF_PEAK_PRICE_PER_KWH = 0.10\n",
        "PEAK_HOURS = {18, 19, 20, 21, 22} # 6 PM to 10 PM\n",
        "\n",
        "# Simulation Time\n",
        "START_HOUR = 18 # 6 PM\n",
        "END_HOUR = 7    # 7 AM\n",
        "TOTAL_HOURS = (24 - START_HOUR) + END_HOUR # Total duration of the simulation\n",
        "\n",
        "# Reward Parameters\n",
        "SUCCESS_REWARD = 100\n",
        "FAILURE_PENALTY = -500\n",
        "\n",
        "class EVChargingEnv(gym.Env):\n",
        "    \"\"\"A custom Gymnasium environment for simulating EV charging.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EVChargingEnv, self).__init__()\n",
        "\n",
        "        # Define the Action Space: 0 for Don't Charge, 1 for Charge \n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Define the State Space: [current_battery_level, current_hour]\n",
        "        # We normalize these values to be between 0 and 1 for the neural network \n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0.0, 0.0]),\n",
        "            high=np.array([1.0, 1.0]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Initialize environment state\n",
        "        self.current_hour = START_HOUR\n",
        "        self.battery_kwh = 0.0 # Assume the car starts with 0 charge\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"Returns the current state, normalized for the agent.\"\"\"\n",
        "        normalized_battery = self.battery_kwh / EV_BATTERY_CAPACITY_KWH\n",
        "        normalized_hour = (self.current_hour - START_HOUR) % 24 / TOTAL_HOURS\n",
        "        return np.array([normalized_battery, normalized_hour], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Resets the environment to its initial state for a new episode.\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.current_hour = START_HOUR\n",
        "        self.battery_kwh = 0.0 # Start with an empty battery\n",
        "        return self._get_state(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Executes one time step within the environment based on the agent's action.\"\"\"\n",
        "        assert self.action_space.contains(action), f\"{action} is an invalid action.\"\n",
        "\n",
        "        cost = 0.0\n",
        "        if action == 1: # If the agent chooses to charge\n",
        "            self.battery_kwh += CHARGER_POWER_KW\n",
        "            self.battery_kwh = min(self.battery_kwh, EV_BATTERY_CAPACITY_KWH)\n",
        "            current_price = PEAK_PRICE_PER_KWH if self.current_hour in PEAK_HOURS else OFF_PEAK_PRICE_PER_KWH\n",
        "            cost = CHARGER_POWER_KW * current_price\n",
        "\n",
        "        # Advance the time by one hour\n",
        "        self.current_hour = (self.current_hour + 1) % 24\n",
        "\n",
        "        # Reward Shaping: Give a small bonus for charging to encourage exploration\n",
        "        charge_increase_bonus = 1.0 if (action == 1 and self.battery_kwh < EV_BATTERY_CAPACITY_KWH) else 0.0\n",
        "        reward = -cost + charge_increase_bonus\n",
        "\n",
        "        # Check if the episode has ended\n",
        "        terminated = self.current_hour == END_HOUR\n",
        "\n",
        "        # At the final time step, apply the large bonus or penalty \n",
        "        if terminated:\n",
        "            if self.battery_kwh >= EV_BATTERY_CAPACITY_KWH * 0.99:\n",
        "                reward += SUCCESS_REWARD # Large bonus for success \n",
        "            else:\n",
        "                reward += FAILURE_PENALTY # Large penalty for failure \n",
        "\n",
        "        return self._get_state(), reward, terminated, False, {}\n",
        "\n",
        "# ==============================================================================\n",
        "# === Part 2: The Agent (ActorCritic)\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "    \"\"\"An Actor-Critic network with increased depth for better feature extraction.\"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # A deeper shared network for more complex feature learning\n",
        "        self.shared_layer_1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.shared_layer_2 = layers.Dense(128, activation=\"relu\")\n",
        "\n",
        "        # The Actor-specific head outputs action probabilities\n",
        "        self.actor_head = layers.Dense(action_dim, activation=\"softmax\")\n",
        "\n",
        "        # The Critic-specific head outputs the estimated state value\n",
        "        self.critic_head = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"Forward pass of the model.\"\"\"\n",
        "        x = self.shared_layer_1(inputs)\n",
        "        shared_features = self.shared_layer_2(x)\n",
        "        action_probs = self.actor_head(shared_features)\n",
        "        state_value = self.critic_head(shared_features)\n",
        "        return action_probs, state_value\n",
        "\n",
        "# ==============================================================================\n",
        "# === Part 3: The Training Loop\n",
        "# ==============================================================================\n",
        "import collections\n",
        "import tqdm # A library to show a progress bar\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Final Hyperparameters ---\n",
        "learning_rate = 0.001\n",
        "gamma = 0.98        # Discount factor\n",
        "gae_lambda = 0.95   # Lambda for Generalized Advantage Estimation\n",
        "max_episodes = 1000 # Reduced for faster training\n",
        "\n",
        "# --- Initialization ---\n",
        "env = EVChargingEnv()\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "agent = ActorCritic(state_dim, action_dim)\n",
        "# Optimizer with gradient clipping for stability\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "# --- Final Training Loop ---\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in tqdm.trange(max_episodes):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # --- 1. Collect Experience for one full episode ---\n",
        "        history_actions = []\n",
        "        history_action_probs = []\n",
        "        history_critic_values = []\n",
        "        history_rewards = []\n",
        "        \n",
        "        state, _ = env.reset()\n",
        "        while True:\n",
        "            state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
        "            action_probs, critic_value = agent(state_tensor)\n",
        "            action = np.random.choice(action_dim, p=np.squeeze(action_probs))\n",
        "            \n",
        "            history_actions.append(action)\n",
        "            history_action_probs.append(action_probs)\n",
        "            history_critic_values.append(critic_value[0, 0])\n",
        "            \n",
        "            state, reward, terminated, _, _ = env.step(action)\n",
        "            history_rewards.append(reward)\n",
        "            \n",
        "            if terminated:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(sum(history_rewards))\n",
        "\n",
        "        # --- 2. Calculate GAE and Returns for the episode ---\n",
        "        advantages = []\n",
        "        returns = []\n",
        "        next_value = 0\n",
        "        \n",
        "        for value, reward in zip(reversed(history_critic_values), reversed(history_rewards)):\n",
        "            td_error = reward + gamma * next_value - value\n",
        "            advantage = td_error + gamma * gae_lambda * (advantages[-1] if advantages else 0)\n",
        "            advantages.append(advantage)\n",
        "            next_value = value\n",
        "        \n",
        "        advantages.reverse()\n",
        "        returns = np.array(advantages) + np.array(history_critic_values)\n",
        "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
        "\n",
        "        # --- 3. Calculate Actor and Critic Losses ---\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "\n",
        "        for i in range(len(history_rewards)):\n",
        "            action = history_actions[i]\n",
        "            prob_dist = history_action_probs[i]\n",
        "            adv = advantages[i]\n",
        "            ret = returns[i]\n",
        "            value = history_critic_values[i]\n",
        "\n",
        "            # Entropy bonus for exploration\n",
        "            entropy = -tf.math.reduce_sum(prob_dist * tf.math.log(prob_dist + 1e-10))\n",
        "            \n",
        "            # Actor loss\n",
        "            prob = prob_dist[0, action]\n",
        "            actor_losses.append(-tf.math.log(prob + 1e-10) * adv - 0.01 * entropy)\n",
        "            \n",
        "            # Critic loss\n",
        "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
        "\n",
        "        total_loss = tf.math.reduce_sum(actor_losses) + tf.math.reduce_sum(critic_losses)\n",
        "\n",
        "    # --- 4. Apply Gradients to update the agent's networks ---\n",
        "    grads = tape.gradient(total_loss, agent.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, agent.trainable_variables))\n",
        "\n",
        "    # Log progress periodically\n",
        "    if episode % 100 == 0 or episode == max_episodes - 1:\n",
        "        avg_reward = np.mean(episode_rewards[-100:])\n",
        "        print(f\"\\nEpisode {episode}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# === Part 4: The Evaluation Script\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "# Use the 'agent' object that has now been trained\n",
        "trained_agent = agent\n",
        "\n",
        "# Run one full episode with the final, trained policy\n",
        "state, _ = env.reset()\n",
        "terminated = False\n",
        "total_reward = 0\n",
        "history = []\n",
        "\n",
        "print(\"\\n--- Running Trained Agent for Evaluation ---\")\n",
        "while not terminated:\n",
        "    state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
        "    action_probs, _ = trained_agent(state_tensor)\n",
        "    \n",
        "    # Choose the BEST action (highest probability), not a random one\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "    \n",
        "    history.append({'hour': env.current_hour, 'battery_kwh': env.battery_kwh, 'action': action})\n",
        "    \n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "history.append({'hour': env.current_hour, 'battery_kwh': env.battery_kwh, 'action': -1})\n",
        "\n",
        "# Calculate the final cost\n",
        "final_cost = -(total_reward - SUCCESS_REWARD) \n",
        "\n",
        "print(f\"Final Battery Level: {env.battery_kwh / EV_BATTERY_CAPACITY_KWH * 100:.2f}%\")\n",
        "print(f\"Total Cost to Full Charge: ${final_cost:.2f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# === Part 5: The Visualization Script\n",
        "# ==============================================================================\n",
        "df = pd.DataFrame(history)\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "# Plot battery level over time\n",
        "ax.plot(df.index, df['battery_kwh'] / EV_BATTERY_CAPACITY_KWH * 100, marker='o', linestyle='-', label='Battery %')\n",
        "\n",
        "# Set labels, title, and grid\n",
        "ax.set_xlabel(\"Time (Hour of Day)\")\n",
        "ax.set_ylabel(\"Battery Level (%)\")\n",
        "ax.set_title(\"Optimal Charging Schedule Learned by A2C Agent\")\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "ax.set_xticks(df.index)\n",
        "ax.set_xticklabels(df['hour'])\n",
        "ax.set_ylim(0, 105)\n",
        "\n",
        "# Color the background to show peak vs. off-peak periods\n",
        "for i in range(len(df) - 1):\n",
        "    hour = df['hour'].iloc[i]\n",
        "    if hour in PEAK_HOURS:\n",
        "        ax.axvspan(i, i + 1, facecolor='red', alpha=0.2, label='Peak Price Period' if i == 0 else \"\")\n",
        "    else:\n",
        "        ax.axvspan(i, i + 1, facecolor='green', alpha=0.2, label='Off-Peak Price Period' if i == 6 else \"\")\n",
        "\n",
        "# Add markers to show when the agent chose to charge\n",
        "charge_points = df[df['action'] == 1]\n",
        "ax.scatter(charge_points.index, charge_points['battery_kwh'] / EV_BATTERY_CAPACITY_KWH * 100, \n",
        "           color='blue', s=150, zorder=5, label='Agent Chose to Charge')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
